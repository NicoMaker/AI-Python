{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Per installare le librerie necessarie, esegui i seguenti comandi:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# pip install keras\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# pip install scikit-learn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# pip install matplotlib\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Per installare le librerie necessarie, esegui i seguenti comandi:\n",
    "\n",
    "# pip install keras\n",
    "# pip install scikit-learn\n",
    "# pip install matplotlib\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importa i dati e crea training e test, dal training ricava il validation set\n",
    "(X_train_val, y_train_val), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15)\n",
    "\n",
    "# Normalizza i valori dei pixel rispetto valore massimo 255\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Inizializza il modello\n",
    "lstm = Sequential()\n",
    "\n",
    "# Aggiunge il livello input a LSTM\n",
    "lstm.add(CuDNNLSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "lstm.add(Dropout(0.2))\n",
    "\n",
    "# Aggiunge un altro livello\n",
    "lstm.add(CuDNNLSTM(128))\n",
    "\n",
    "# Aggiunge il dense hidden layer con attivazione relu\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "lstm.add(Dropout(0.2))\n",
    "\n",
    "# Aggiunge livello output\n",
    "lstm.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compila il modello con i parametri per training e misura metrica\n",
    "lstm.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "# Fit del modello con i dati per fare training\n",
    "history = lstm.fit(X_train, y_train, epochs=3, validation_data=(X_val, y_val))\n",
    "\n",
    "# Raccoglie i dati per descrivere il training eseguito\n",
    "print(history.history.keys())\n",
    "\n",
    "# Crea i grafici per mostrare andamento del training eseguito\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Andamento loss durante il training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoca')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Valuta i dati sul test set e misura delle metriche\n",
    "test_loss, test_acc = lstm.evaluate(X_test, y_test)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Flow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x  # attiva esecuzione della versione 2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# crea un tensore\n",
    "x = [[2.]]\n",
    "m = tf.square(x)\n",
    "print(\"Descrizione di m:\", m)\n",
    "print(\"Contenuto di m:\", m.numpy())\n",
    "\n",
    "# moltiplica due tensori\n",
    "a = tf.constant([[2, 2], [3, 4]])\n",
    "b = tf.constant([[2, 1],  # separare le righe aumenta leggibilità\n",
    "                 [3, 2]])\n",
    "ab = tf.matmul(a, b)\n",
    "print('a * b = \\n', ab.numpy())\n",
    "\n",
    "@tf.function  # definisce operazione su tensore\n",
    "def f(x): return tf.add(x, 1.)  # aggiunge 1 a tensore x\n",
    "\n",
    "a = tf.constant(\"Benvenuto TensorFlow 2.0!\")  # tensore con una stringa\n",
    "print(a)\n",
    "print(a.numpy())\n",
    "\n",
    "scalar = tf.constant(1.0)  # tensore contiene una costante\n",
    "vector = tf.constant([1.0, 1.0])  # tensore come array con 2 costanti\n",
    "\n",
    "matrix = tf.constant([[3.0, 2.0], [6.0, 4.0]])  # tensore come matrice 2x2 con 4 costanti\n",
    "print(f(scalar))\n",
    "print(f(vector))\n",
    "print(f(matrix))\n",
    "\n",
    "# calcolo del grafo in Figura 18.1\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(8)\n",
    "c = tf.constant(2)\n",
    "d = tf.constant(3)\n",
    "e = tf.constant(6)\n",
    "f = ((a + b + c) * d) / e\n",
    "print(f)\n",
    "print(f.numpy())\n",
    "\n",
    "# crea un computational graph con i due approcci\n",
    "import numpy as np\n",
    "data = np.array([[1.764, 0.400],\n",
    "                 [0.978, 2.240],\n",
    "                 [1.867, -0.977],\n",
    "                 [2.467, -0.107]])\n",
    "\n",
    "input_layer = tf.keras.layers.InputLayer(input_shape=(2))  # input layer con 2 valori\n",
    "hidden = tf.keras.layers.Dense(3, activation='relu')  # 1 hidden layer denso con 3 neuroni\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')  # output layer denso con 1 neurone\n",
    "\n",
    "# crea modello con approccio di chiamata a funzione ed esecuzione dinamica\n",
    "# i dati di input vanno nel primo livello, che è il primo nodo\n",
    "# quando aggiunge il secondo nodo, l'output del primo nodo viene inserito nel secondo nodo\n",
    "# e viene calcolato l'output del secondo nodo e così via\n",
    "# consente di stampare stati intermedi del modello, ma rallenta molto i calcoli\n",
    "def model_1(data):\n",
    "    x = hidden(data)\n",
    "    print('Dopo il primo layer:', x)\n",
    "    out = output(x)\n",
    "    print('Dopo il secondo layer:', out)\n",
    "    return out\n",
    "\n",
    "print('Output come tensore:', model_1(data))\n",
    "print('Output come valori:\\n', model_1(data).numpy())\n",
    "\n",
    "@tf.function  # crea modello con approccio creazione grafo statico\n",
    "# prima collega tutti i nodi facendo una grande operazione computazionale\n",
    "# quindi segue il flusso del grafo per fare i calcoli, non mostra stati intermedi\n",
    "# del modello né aggiunge nodi al volo, ma è più veloce dell'altro approccio\n",
    "def model_2(data):\n",
    "    x = input_layer(data)\n",
    "    x = hidden(x)\n",
    "    print('Dopo il primo layer:', x)\n",
    "    out = output(x)\n",
    "    print('Dopo il secondo layer:', out)\n",
    "    return out\n",
    "\n",
    "print('Output come tensore:', model_2(data))\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    print('batch ciclo esterno {} interno {}:'.format(i, j))\n",
    "    model_1(d[np.newaxis, :])  # calcola loss con approccio eager\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    print('batch ciclo esterno {} interno {}:'.format(i, j))\n",
    "    model_2(d[np.newaxis, :])  # calcola loss con approccio grafo statico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Flow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElenco data set in TensorFlow 2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, tfds\u001b[38;5;241m.\u001b[39mlist_builders())\n\u001b[0;32m      3\u001b[0m data, info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_mnist\u001b[39m\u001b[38;5;124m'\u001b[39m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"Elenco data set in TensorFlow 2.0\", tfds.list_builders())\n",
    "data, info = tfds.load(name='fashion_mnist', as_supervised=True, split=None, with_info=True)\n",
    "print(\"Descrizione data set MNIST\",info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
